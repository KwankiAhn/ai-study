{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch hello world\n",
    "* https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1000]), torch.Size([64, 10]), torch.Size([1000]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 31.906471252441406\n",
      "199 0.2842945158481598\n",
      "299 0.002175460569560528\n",
      "399 7.674582775507588e-06\n",
      "499 8.774386550669533e-09\n"
     ]
    }
   ],
   "source": [
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complex type tensor\n",
    "\n",
    "- not supported yet : https://github.com/pytorch/pytorch/issues/755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.complex128. The only supported types are: double, float, float16, int64, int32, and uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-a755103f0830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1j\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3j\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0math\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mbth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0math_cuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0math\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.complex128. The only supported types are: double, float, float16, int64, int32, and uint8."
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1+1j,2+2j])\n",
    "b = np.array([3+3j,4+4j])\n",
    "ath = th.from_numpy(a)\n",
    "bth = th.from_numpy(b)\n",
    "ath_cuda = ath.cuda()\n",
    "ath_cuda += bth.cuda()\n",
    "ath = ath_cuda.cpu()\n",
    "print(ath.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imitate rx simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.7443105126095386+0.0031990149596619055j)\n"
     ]
    }
   ],
   "source": [
    "def gen_rand_complex(low, high):\n",
    "    r = np.random.uniform(low, high)\n",
    "    i = np.random.uniform(low, high)\n",
    "    return r + 1j * i\n",
    "\n",
    "# UNIT TEST\n",
    "t_value = gen_rand_complex(-1, 1)\n",
    "print(t_value)\n",
    "assert type(t_value) is complex, \"error, type should be complex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.learning_rate = 1e-4\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "m = Model(2,2)\n",
    "print(m)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........train, loss : 0.4526260495185852\n",
      ".........train, loss : 0.38767004013061523\n",
      ".........train, loss : 0.44828706979751587\n",
      ".........train, loss : 0.5068626403808594\n",
      ".........train, loss : 0.40768566727638245\n",
      ".........train, loss : 0.2955981492996216\n",
      ".........train, loss : 0.5194939374923706\n",
      ".........train, loss : 0.3902595639228821\n",
      ".........train, loss : 0.6379255056381226\n",
      "........train, loss : 0.23576787114143372\n"
     ]
    }
   ],
   "source": [
    "tx_buf = []\n",
    "rx_buf = []\n",
    "train_batch_size = 10\n",
    "iteration = 100\n",
    "for i in range(iteration):\n",
    "    tx = gen_rand_complex(-1, 1)\n",
    "    rx = gen_rand_complex(-1, 1)\n",
    "    if ((0 < i) and (i % train_batch_size == 0)) or (i == iteration - 1):\n",
    "        # train\n",
    "        x = torch.tensor([[x.real, x.imag] for x in rx_buf])\n",
    "        y = torch.tensor([[x.real, x.imag] for x in tx_buf])\n",
    "        y_pred = m.forward(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"train, loss : {}\".format(loss))\n",
    "        tx_buf = []\n",
    "        rx_buf = []\n",
    "    else:\n",
    "        # save on buf\n",
    "        tx_buf.append(tx)\n",
    "        rx_buf.append(rx)\n",
    "        print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lstm model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLstm(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(ModelLstm, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.learning_rate = 1e-4\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "m = Model(2,2)\n",
    "print(m)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2627,  0.8289,  0.5312]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1.1286, 0.2937, 0.8111]]),\n",
       " tensor([[-0.0121, -0.3044, -0.5591]]),\n",
       " tensor([[-0.6684,  0.4180, -0.2707]]),\n",
       " tensor([[ 0.1936, -1.6071, -0.5119]]),\n",
       " tensor([[-0.1908,  0.5175,  1.9192]])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1286,  0.2937,  0.8111],\n",
       "        [-0.0121, -0.3044, -0.5591],\n",
       "        [-0.6684,  0.4180, -0.2707],\n",
       "        [ 0.1936, -1.6071, -0.5119],\n",
       "        [-0.1908,  0.5175,  1.9192]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1286,  0.2937,  0.8111]],\n",
       "\n",
       "        [[-0.0121, -0.3044, -0.5591]],\n",
       "\n",
       "        [[-0.6684,  0.4180, -0.2707]],\n",
       "\n",
       "        [[ 0.1936, -1.6071, -0.5119]],\n",
       "\n",
       "        [[-0.1908,  0.5175,  1.9192]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(inputs).view(len(inputs), 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2215,  0.2781,  0.1470]],\n",
      "\n",
      "        [[ 0.0348,  0.2484,  0.0457]],\n",
      "\n",
      "        [[-0.0827,  0.0845,  0.0975]],\n",
      "\n",
      "        [[-0.0141, -0.0733,  0.0782]],\n",
      "\n",
      "        [[-0.1010, -0.1885,  0.0506]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.1010, -0.1885,  0.0506]]], grad_fn=<StackBackward>), tensor([[[-0.1664, -0.2707,  0.1635]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "print(out)\n",
    "print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
