{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch hello world\n",
    "* https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1000]), torch.Size([64, 10]), torch.Size([1000]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 67.84065246582031\n",
      "199 1.6440156698226929\n",
      "299 0.025622230023145676\n",
      "399 0.0001580471871420741\n",
      "499 4.235481299019739e-07\n"
     ]
    }
   ],
   "source": [
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## complex type tensor\n",
    "\n",
    "- not supported yet : https://github.com/pytorch/pytorch/issues/755"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.complex128. The only supported types are: double, float, float16, int64, int32, and uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-a755103f0830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1j\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m2j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3j\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0math\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mbth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0math_cuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0math\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.complex128. The only supported types are: double, float, float16, int64, int32, and uint8."
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1+1j,2+2j])\n",
    "b = np.array([3+3j,4+4j])\n",
    "ath = th.from_numpy(a)\n",
    "bth = th.from_numpy(b)\n",
    "ath_cuda = ath.cuda()\n",
    "ath_cuda += bth.cuda()\n",
    "ath = ath_cuda.cpu()\n",
    "print(ath.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imitate rx simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.16987431961826283+0.41533789439253366j)\n"
     ]
    }
   ],
   "source": [
    "def gen_rand_complex(low, high):\n",
    "    r = np.random.uniform(low, high)\n",
    "    i = np.random.uniform(low, high)\n",
    "    return r + 1j * i\n",
    "\n",
    "# UNIT TEST\n",
    "t_value = gen_rand_complex(-1, 1)\n",
    "print(t_value)\n",
    "assert type(t_value) is complex, \"error, type should be complex\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummaryX\n",
      "  Downloading https://files.pythonhosted.org/packages/36/23/87eeaaf70daa61aa21495ece0969c50c446b8fd42c4b8905af264b40fe7f/torchsummaryX-1.3.0-py3-none-any.whl\n",
      "Requirement already satisfied: torch in c:\\users\\pray4\\anaconda3\\lib\\site-packages (from torchsummaryX) (1.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\pray4\\anaconda3\\lib\\site-packages (from torchsummaryX) (1.16.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\pray4\\anaconda3\\lib\\site-packages (from torchsummaryX) (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\pray4\\anaconda3\\lib\\site-packages (from pandas->torchsummaryX) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\pray4\\anaconda3\\lib\\site-packages (from pandas->torchsummaryX) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pray4\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas->torchsummaryX) (1.12.0)\n",
      "Installing collected packages: torchsummaryX\n",
      "Successfully installed torchsummaryX-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummaryX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.learning_rate = 1e-4\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "m = Model(2,2)\n",
    "print(m)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================\n",
      "                 Kernel Shape Output Shape Params Mult-Adds\n",
      "Layer                                                      \n",
      "0_model.Linear_0      [2, 64]      [1, 64]  192.0     128.0\n",
      "1_model.ReLU_1              -      [1, 64]      -         -\n",
      "2_model.Linear_2      [64, 2]       [1, 2]  130.0     128.0\n",
      "3_model.ReLU_3              -       [1, 2]      -         -\n",
      "-------------------------------------------------------------\n",
      "                      Totals\n",
      "Total params           322.0\n",
      "Trainable params       322.0\n",
      "Non-trainable params     0.0\n",
      "Mult-Adds              256.0\n",
      "=============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_model.Linear_0</th>\n",
       "      <td>[2, 64]</td>\n",
       "      <td>[1, 64]</td>\n",
       "      <td>192.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_model.ReLU_1</th>\n",
       "      <td>-</td>\n",
       "      <td>[1, 64]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_model.Linear_2</th>\n",
       "      <td>[64, 2]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>130.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_model.ReLU_3</th>\n",
       "      <td>-</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Kernel Shape Output Shape  Params  Mult-Adds\n",
       "Layer                                                        \n",
       "0_model.Linear_0      [2, 64]      [1, 64]   192.0      128.0\n",
       "1_model.ReLU_1              -      [1, 64]     NaN        NaN\n",
       "2_model.Linear_2      [64, 2]       [1, 2]   130.0      128.0\n",
       "3_model.ReLU_3              -       [1, 2]     NaN        NaN"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "summary(m, torch.zeros((1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "x=tensor([[ 0.4426, -0.7361],\n",
      "        [-0.9835,  0.8398],\n",
      "        [ 0.0394,  0.5972],\n",
      "        [ 0.8498, -0.1154],\n",
      "        [-0.2868,  0.0730],\n",
      "        [-0.4126, -0.4304],\n",
      "        [ 0.9563, -0.6813],\n",
      "        [-0.5588,  0.1995],\n",
      "        [-0.0563, -0.7030],\n",
      "        [-0.8544, -0.3600]])\n",
      "\n",
      "y_pred=tensor([[0.0440, 0.0000],\n",
      "        [0.0000, 0.1325],\n",
      "        [0.1757, 0.0593],\n",
      "        [0.1973, 0.0000],\n",
      "        [0.0000, 0.0010],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.1404, 0.0000],\n",
      "        [0.0000, 0.0258],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.368866890668869\n",
      ".........\n",
      "x=tensor([[-0.4975, -0.3638],\n",
      "        [ 0.4995,  0.0866],\n",
      "        [ 0.2356, -0.5844],\n",
      "        [-0.2909, -0.1573],\n",
      "        [ 0.9139,  0.3674],\n",
      "        [-0.0378,  0.5091],\n",
      "        [ 0.0611,  0.6584],\n",
      "        [ 0.5415, -0.7570],\n",
      "        [-0.8469,  0.9909]])\n",
      "\n",
      "y_pred=tensor([[0.0000, 0.0000],\n",
      "        [0.1867, 0.0000],\n",
      "        [0.0068, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2904, 0.0000],\n",
      "        [0.1389, 0.0555],\n",
      "        [0.1910, 0.0632],\n",
      "        [0.0663, 0.0000],\n",
      "        [0.0000, 0.1518]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.25408515334129333\n",
      ".........\n",
      "x=tensor([[ 0.6734,  0.2343],\n",
      "        [ 0.0753, -0.3089],\n",
      "        [-0.6997, -0.5722],\n",
      "        [-0.5166,  0.1016],\n",
      "        [ 0.3714, -0.7785],\n",
      "        [-0.0780, -0.3078],\n",
      "        [-0.7625,  0.6647],\n",
      "        [ 0.1228,  0.2295],\n",
      "        [-0.5242, -0.4710]])\n",
      "\n",
      "y_pred=tensor([[0.2380, 0.0000],\n",
      "        [0.0145, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0022],\n",
      "        [0.0248, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.1018],\n",
      "        [0.1255, 0.0009],\n",
      "        [0.0000, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.26074373722076416\n",
      ".........\n",
      "x=tensor([[ 0.5223,  0.7921],\n",
      "        [-0.7977, -0.7829],\n",
      "        [ 0.2050,  0.1047],\n",
      "        [ 0.9943,  0.2582],\n",
      "        [ 0.5438, -0.5870],\n",
      "        [-0.5842,  0.9550],\n",
      "        [ 0.8821,  0.4977],\n",
      "        [-0.0189, -0.7366],\n",
      "        [ 0.5916, -0.6855]])\n",
      "\n",
      "y_pred=tensor([[0.3030, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.1202, 0.0000],\n",
      "        [0.2744, 0.0000],\n",
      "        [0.0711, 0.0000],\n",
      "        [0.0448, 0.1410],\n",
      "        [0.3111, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0814, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.3284098207950592\n",
      ".........\n",
      "x=tensor([[ 0.1046,  0.4029],\n",
      "        [ 0.3677, -0.1657],\n",
      "        [-0.4759, -0.9166],\n",
      "        [-0.0853, -0.8956],\n",
      "        [-0.1660, -0.9184],\n",
      "        [-0.6483,  0.4245],\n",
      "        [-0.2154,  0.7716],\n",
      "        [ 0.2971,  0.6469],\n",
      "        [ 0.0052,  0.8377]])\n",
      "\n",
      "y_pred=tensor([[0.1523, 0.0252],\n",
      "        [0.1012, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0625],\n",
      "        [0.1389, 0.1048],\n",
      "        [0.2376, 0.0255],\n",
      "        [0.2072, 0.0952]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.3767447769641876\n",
      ".........\n",
      "x=tensor([[-0.9335,  0.5231],\n",
      "        [-0.5035, -0.4272],\n",
      "        [-0.4126,  0.8970],\n",
      "        [ 0.7583,  0.2913],\n",
      "        [ 0.9894, -0.7097],\n",
      "        [ 0.3124, -0.7019],\n",
      "        [-0.3488, -0.8194],\n",
      "        [ 0.1433,  0.9955],\n",
      "        [-0.6618,  0.8574]])\n",
      "\n",
      "y_pred=tensor([[0.0000, 0.0748],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0924, 0.1312],\n",
      "        [0.2540, 0.0000],\n",
      "        [0.1399, 0.0000],\n",
      "        [0.0103, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2558, 0.0830],\n",
      "        [0.0034, 0.1274]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.43806201219558716\n",
      ".........\n",
      "x=tensor([[ 0.4057,  0.4516],\n",
      "        [-0.2574,  0.8837],\n",
      "        [-0.1571, -0.8648],\n",
      "        [ 0.8508,  0.1103],\n",
      "        [-0.7732,  0.0725],\n",
      "        [-0.9841, -0.8757],\n",
      "        [ 0.0662, -0.1393],\n",
      "        [-0.0920, -0.3536],\n",
      "        [ 0.2875,  0.8728]])\n",
      "\n",
      "y_pred=tensor([[0.2248, 0.0000],\n",
      "        [0.1394, 0.1218],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2336, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0419, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2655, 0.0406]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.3444032371044159\n",
      ".........\n",
      "x=tensor([[-0.8586,  0.2235],\n",
      "        [-0.4738, -0.0019],\n",
      "        [ 0.6653,  0.4139],\n",
      "        [ 0.8366,  0.0134],\n",
      "        [ 0.0693,  0.8553],\n",
      "        [ 0.4938, -0.0989],\n",
      "        [-0.9648,  0.7515],\n",
      "        [-0.1932,  0.7672],\n",
      "        [-0.4856, -0.3407]])\n",
      "\n",
      "y_pred=tensor([[0.0000, 0.0084],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2632, 0.0000],\n",
      "        [0.2145, 0.0000],\n",
      "        [0.2210, 0.0887],\n",
      "        [0.1433, 0.0000],\n",
      "        [0.0000, 0.1158],\n",
      "        [0.1415, 0.1020],\n",
      "        [0.0000, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.3208007514476776\n",
      ".........\n",
      "x=tensor([[-0.9131, -0.5852],\n",
      "        [ 0.6725,  0.9664],\n",
      "        [-0.7455, -0.4999],\n",
      "        [ 0.4963,  0.0568],\n",
      "        [ 0.0777,  0.7605],\n",
      "        [-0.4687, -0.6413],\n",
      "        [ 0.7777,  0.1759],\n",
      "        [-0.7197, -0.6058],\n",
      "        [-0.3072, -0.3385]])\n",
      "\n",
      "y_pred=tensor([[0.0000, 0.0000],\n",
      "        [0.3441, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.1760, 0.0000],\n",
      "        [0.2069, 0.0741],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2357, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.3880077004432678\n",
      "........\n",
      "x=tensor([[-0.0140,  0.1749],\n",
      "        [ 0.2947, -0.3458],\n",
      "        [ 0.3186,  0.3257],\n",
      "        [ 0.5116,  0.9110],\n",
      "        [ 0.5542, -0.5179],\n",
      "        [-0.4767, -0.9384],\n",
      "        [ 0.4748, -0.3823],\n",
      "        [ 0.7410,  0.3742]])\n",
      "\n",
      "y_pred=tensor([[0.0755, 0.0067],\n",
      "        [0.0451, 0.0000],\n",
      "        [0.1862, 0.0000],\n",
      "        [0.3082, 0.0000],\n",
      "        [0.0725, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0742, 0.0000],\n",
      "        [0.2634, 0.0000]], grad_fn=<ThresholdBackward0>)\n",
      "train, loss : 0.47490158677101135\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# Simulator\n",
    "# |- iteration = every clock, receive single signal\n",
    "####################################################################################\n",
    "\n",
    "tx_buf = []\n",
    "rx_buf = []\n",
    "train_batch_size = 10\n",
    "iteration = 100\n",
    "\n",
    "for i in range(iteration):\n",
    "    tx = gen_rand_complex(-1, 1)\n",
    "    rx = gen_rand_complex(-1, 1)\n",
    "    if ((0 < i) and (i % train_batch_size == 0)) or (i == iteration - 1):\n",
    "        # train\n",
    "        x = torch.tensor([[x.real, x.imag] for x in rx_buf])\n",
    "        y = torch.tensor([[x.real, x.imag] for x in tx_buf])\n",
    "        y_pred = m.forward(x)\n",
    "        print(\"\\nx={}\".format(x))\n",
    "        print(\"\\ny_pred={}\".format(y_pred))\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"train, loss : {}\".format(loss))\n",
    "        tx_buf = []\n",
    "        rx_buf = []\n",
    "    else:\n",
    "        # save on buf\n",
    "        tx_buf.append(tx)\n",
    "        rx_buf.append(rx)\n",
    "        print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3994, 0.0000]], grad_fn=<ThresholdBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x = torch.tensor([[1.,1.]])\n",
    "m.forward(tmp_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 2\n",
    "input_size = 2 # one-hot size \n",
    "hidden_size = 3 #output from the cell. 바로 결과 예측을 위해서 5로 설정 \n",
    "num_layers = 1 # one layer rnn이다. (아직 안 다룬 개념)\n",
    "sequence_length = 4\n",
    "\n",
    "feature_size = 2\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Model,self).__init__() \n",
    "        self.rnn=nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first=True) \n",
    "        self.hidden2tag = nn.Linear(hidden_size, target_size)\n",
    "        self.activate = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, hidden): \n",
    "        #input x 를 (batch_size,sequence_length,input_size)로 reshape함 \n",
    "        #just for make sure이라고 하심. \n",
    "        x=x.view(batch_size, sequence_length, input_size) \n",
    "        \n",
    "        #Propagate input through RNN \n",
    "        #Input:(batch,seq_len,input_size) \n",
    "        out, _ = self.rnn(x, hidden)\n",
    "        out = self.hidden2tag(out)\n",
    "        #out = F.log_softmax(out, dim=1)\n",
    "        out = self.activate(out)\n",
    "        #for make sure, output이 N * 5 shape을 따르게 하기 위해서 \n",
    "        out = out.view(-1, target_size) \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self): \n",
    "        #initialize hidden and cell states \n",
    "        return torch.zeros(num_layers, batch_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "model = Model().to(device) \n",
    "\n",
    "loss_fn = nn.MSELoss() \n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.1) \n",
    "\n",
    "# inputs = inputs.to(device) \n",
    "# labels = labels.to(device)\n",
    "\n",
    "# for epoch in range(100): \n",
    "#     optimizer.zero_grad() \n",
    "#     loss = 0 \n",
    "#     hidden = model.init_hidden() \n",
    "    \n",
    "#     # print(\"predicted string : \",end=\"\")\n",
    "\n",
    "#     print(\"inputs : {}, labels : {}\".format(inputs, labels))\n",
    "#     outputs = model(inputs,hidden) \n",
    "#     print(\"outputs : {}\".format(outputs))\n",
    "#     loss = criterion(outputs,labels) \n",
    "#     _, idx = outputs.max(1) \n",
    "#     print(\"idx : {}\".format(idx))\n",
    "#     result_str = [idx2char[c] for c in idx.squeeze()] \n",
    "#     #print(\"outputs_sq : {}\".format(outputs_sq))\n",
    "#     print(\"epoch: %d, loss: %1.3f\" % (epoch+1,loss.data)) \n",
    "#     print(\"Predicted string: \", ''.join(result_str)) \n",
    "#     loss.backward() \n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7 % 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............iteration : 12, tx_buf len : 12, rx_buf len : 12\n",
      "trying view x (12x2) as size of(3,4,2)\n",
      "\n",
      "x=tensor([[[-0.9740, -0.8861],\n",
      "         [-0.2088, -0.8035],\n",
      "         [ 0.2793, -0.4247],\n",
      "         [-0.9584,  0.3434]],\n",
      "\n",
      "        [[ 0.5459,  0.4039],\n",
      "         [ 0.2815, -0.5074],\n",
      "         [-0.6237, -0.7578],\n",
      "         [ 0.8923, -0.8074]],\n",
      "\n",
      "        [[-0.4334, -0.4539],\n",
      "         [ 0.9149,  0.4581],\n",
      "         [ 0.1481, -0.0930],\n",
      "         [-0.3368, -0.1613]]])\n",
      "\n",
      "y_pred=tensor([[-0.2201,  0.3293],\n",
      "        [-0.0118,  0.3472],\n",
      "        [ 0.1980,  0.1116],\n",
      "        [-0.2522, -0.0252],\n",
      "        [ 0.1400, -0.0583],\n",
      "        [ 0.1503,  0.3029],\n",
      "        [ 0.0152,  0.2127],\n",
      "        [ 0.3194,  0.2847],\n",
      "        [-0.0903,  0.2036],\n",
      "        [ 0.2316, -0.0217],\n",
      "        [ 0.1301,  0.1349],\n",
      "        [-0.0143,  0.1454]], grad_fn=<ViewBackward>)\n",
      "train, loss : 0.41503122448921204\n",
      "............iteration : 25, tx_buf len : 12, rx_buf len : 12\n",
      "trying view x (12x2) as size of(3,4,2)\n",
      "\n",
      "x=tensor([[[ 0.7144, -0.4837],\n",
      "         [-0.1516,  0.0497],\n",
      "         [-0.2042, -0.1590],\n",
      "         [ 0.1172,  0.2964]],\n",
      "\n",
      "        [[ 0.3938,  0.4198],\n",
      "         [ 0.4432, -0.9431],\n",
      "         [ 0.9739,  0.6214],\n",
      "         [-0.6042, -0.8168]],\n",
      "\n",
      "        [[ 0.7888,  0.3958],\n",
      "         [ 0.9629, -0.0562],\n",
      "         [ 0.7152, -0.2137],\n",
      "         [ 0.5768, -0.5636]]])\n",
      "\n",
      "y_pred=tensor([[ 0.4887, -0.1341],\n",
      "        [ 0.4616, -0.3155],\n",
      "        [ 0.4573, -0.1840],\n",
      "        [ 0.4710, -0.3388],\n",
      "        [ 0.3328, -0.3308],\n",
      "        [ 0.5210,  0.0737],\n",
      "        [ 0.5823, -0.4899],\n",
      "        [ 0.4795,  0.0153],\n",
      "        [ 0.4229, -0.3375],\n",
      "        [ 0.5697, -0.2100],\n",
      "        [ 0.5966, -0.2987],\n",
      "        [ 0.5885, -0.1882]], grad_fn=<ViewBackward>)\n",
      "train, loss : 1.2178338766098022\n",
      "............iteration : 38, tx_buf len : 12, rx_buf len : 12\n",
      "trying view x (12x2) as size of(3,4,2)\n",
      "\n",
      "x=tensor([[[ 0.1734, -0.7851],\n",
      "         [-0.7156,  0.3173],\n",
      "         [ 0.6518,  0.6346],\n",
      "         [-0.9309, -0.7796]],\n",
      "\n",
      "        [[ 0.3032, -0.1820],\n",
      "         [ 0.4434,  0.8204],\n",
      "         [-0.5728,  0.3316],\n",
      "         [-0.2672, -0.6668]],\n",
      "\n",
      "        [[-0.6704,  0.9157],\n",
      "         [-0.0654, -0.5108],\n",
      "         [-0.5965, -0.3111],\n",
      "         [ 0.9880,  0.8590]]])\n",
      "\n",
      "y_pred=tensor([[ 0.4326, -0.4086],\n",
      "        [ 0.3690, -0.5845],\n",
      "        [ 0.4843, -0.5617],\n",
      "        [ 0.4927, -0.4759],\n",
      "        [ 0.3955, -0.5095],\n",
      "        [ 0.4436, -0.5988],\n",
      "        [ 0.4115, -0.5357],\n",
      "        [ 0.5156, -0.4840],\n",
      "        [ 0.0189, -0.5367],\n",
      "        [ 0.4300, -0.2559],\n",
      "        [ 0.4619, -0.5898],\n",
      "        [ 0.5144, -0.6206]], grad_fn=<ViewBackward>)\n",
      "train, loss : 1.0908452272415161\n",
      "............iteration : 51, tx_buf len : 12, rx_buf len : 12\n",
      "trying view x (12x2) as size of(3,4,2)\n",
      "\n",
      "x=tensor([[[ 0.5738,  0.0815],\n",
      "         [ 0.1104, -0.9154],\n",
      "         [-0.3950, -0.7139],\n",
      "         [-0.3220,  0.0335]],\n",
      "\n",
      "        [[ 0.4012, -0.7254],\n",
      "         [-0.2989, -0.6216],\n",
      "         [ 0.4972, -0.1321],\n",
      "         [ 0.6242, -0.8108]],\n",
      "\n",
      "        [[ 0.9638, -0.3363],\n",
      "         [-0.0321,  0.3275],\n",
      "         [-0.3870,  0.3481],\n",
      "         [-0.6310,  0.4457]]])\n",
      "\n",
      "y_pred=tensor([[ 0.2532, -0.4449],\n",
      "        [ 0.4190, -0.3594],\n",
      "        [ 0.3995, -0.4955],\n",
      "        [ 0.3589, -0.4906],\n",
      "        [ 0.3316, -0.3657],\n",
      "        [ 0.3866, -0.4555],\n",
      "        [ 0.4094, -0.4966],\n",
      "        [ 0.4435, -0.4490],\n",
      "        [ 0.3337, -0.4140],\n",
      "        [ 0.3207, -0.4921],\n",
      "        [ 0.3018, -0.4513],\n",
      "        [ 0.2536, -0.4546]], grad_fn=<ViewBackward>)\n",
      "train, loss : 0.6510517001152039\n",
      "............iteration : 64, tx_buf len : 12, rx_buf len : 12\n",
      "trying view x (12x2) as size of(3,4,2)\n",
      "\n",
      "x=tensor([[[ 0.7909, -0.9832],\n",
      "         [-0.4993, -0.0933],\n",
      "         [ 0.7984,  0.8669],\n",
      "         [-0.7680,  0.6447]],\n",
      "\n",
      "        [[-0.5019,  0.0166],\n",
      "         [-0.0132,  0.0830],\n",
      "         [-0.3067, -0.8651],\n",
      "         [ 0.8696, -0.7758]],\n",
      "\n",
      "        [[-0.1680, -0.4817],\n",
      "         [-0.7181, -0.6263],\n",
      "         [-0.1032,  0.2686],\n",
      "         [-0.2322,  0.0696]]])\n",
      "\n",
      "y_pred=tensor([[ 0.2854, -0.1926],\n",
      "        [ 0.2466, -0.3031],\n",
      "        [ 0.2133, -0.2265],\n",
      "        [ 0.1274, -0.2437],\n",
      "        [ 0.1039, -0.2590],\n",
      "        [ 0.2468, -0.1690],\n",
      "        [ 0.3155, -0.2499],\n",
      "        [ 0.3449, -0.2583],\n",
      "        [ 0.2019, -0.2357],\n",
      "        [ 0.2897, -0.2199],\n",
      "        [ 0.2356, -0.2980],\n",
      "        [ 0.2655, -0.2298]], grad_fn=<ViewBackward>)\n",
      "train, loss : 0.047438837587833405\n",
      "............iteration : 77, tx_buf len : 12, rx_buf len : 12\n",
      "trying view x (12x2) as size of(3,4,2)\n",
      "\n",
      "x=tensor([[[ 0.8464, -0.1321],\n",
      "         [-0.5913,  0.7393],\n",
      "         [ 0.8578, -0.2081],\n",
      "         [ 0.4490,  0.7560]],\n",
      "\n",
      "        [[-0.3967,  0.9271],\n",
      "         [ 0.8601, -0.5973],\n",
      "         [ 0.8786, -0.8920],\n",
      "         [-0.1835, -0.2253]],\n",
      "\n",
      "        [[ 0.9075, -0.1682],\n",
      "         [ 0.6037,  0.2743],\n",
      "         [ 0.4614,  0.2794],\n",
      "         [ 0.3841,  0.2366]]])\n",
      "\n",
      "y_pred=tensor([[ 0.1576, -0.1400],\n",
      "        [ 0.0966, -0.0941],\n",
      "        [ 0.2651, -0.0801],\n",
      "        [ 0.1454, -0.1611],\n",
      "        [-0.0713, -0.1331],\n",
      "        [ 0.2982,  0.0227],\n",
      "        [ 0.2559, -0.2196],\n",
      "        [ 0.2539, -0.1408],\n",
      "        [ 0.1641, -0.1391],\n",
      "        [ 0.2161, -0.0974],\n",
      "        [ 0.2104, -0.1413],\n",
      "        [ 0.2223, -0.1217]], grad_fn=<ViewBackward>)\n",
      "train, loss : 0.2344997078180313\n",
      "............iteration : 90, tx_buf len : 12, rx_buf len : 12\n",
      "trying view x (12x2) as size of(3,4,2)\n",
      "\n",
      "x=tensor([[[-0.2260,  0.6289],\n",
      "         [ 0.9321, -0.7297],\n",
      "         [-0.6489, -0.8477],\n",
      "         [ 0.3236, -0.6114]],\n",
      "\n",
      "        [[ 0.2484,  0.6707],\n",
      "         [ 0.6869,  0.8082],\n",
      "         [-0.5579,  0.2778],\n",
      "         [ 0.1459,  0.1135]],\n",
      "\n",
      "        [[-0.4673, -0.0380],\n",
      "         [ 0.3105, -0.3028],\n",
      "         [-0.6233, -0.7303],\n",
      "         [ 0.4272, -0.9960]]])\n",
      "\n",
      "y_pred=tensor([[-0.0542,  0.0061],\n",
      "        [ 0.2128,  0.1016],\n",
      "        [ 0.1386, -0.0969],\n",
      "        [ 0.2036,  0.0462],\n",
      "        [-0.0503,  0.0225],\n",
      "        [ 0.0393,  0.1444],\n",
      "        [ 0.0606, -0.0054],\n",
      "        [ 0.1413,  0.0674],\n",
      "        [ 0.0341, -0.0308],\n",
      "        [ 0.1829,  0.0748],\n",
      "        [ 0.1472, -0.0743],\n",
      "        [ 0.2025,  0.0246]], grad_fn=<ViewBackward>)\n",
      "train, loss : 0.3406471312046051\n",
      "........."
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# Simulator\n",
    "# |- iteration = every clock, receive single signal\n",
    "####################################################################################\n",
    "\n",
    "tx_buf = []\n",
    "rx_buf = []\n",
    "iteration = 100\n",
    "aggregation_size = batch_size * sequence_length\n",
    "for i in range(iteration):\n",
    "    tx = gen_rand_complex(-1, 1)\n",
    "    rx = gen_rand_complex(-1, 1)\n",
    "    #if ((0 < i) and (i % aggregation_size == 0)) or (i == iteration - 1):\n",
    "    if len(tx_buf) >= batch_size * sequence_length: \n",
    "        print(\"iteration : {}, tx_buf len : {}, rx_buf len : {}\".format(i, len(tx_buf), len(rx_buf)))\n",
    "        # train\n",
    "        x = torch.tensor([[x.real, x.imag] for x in rx_buf])\n",
    "        y = torch.tensor([[x.real, x.imag] for x in tx_buf])\n",
    "        #y = torch.tensor([tx_buf[-1].real, tx_buf[-1].imag])\n",
    "        try:\n",
    "            # buffer is not enough\n",
    "            print(\"trying view x ({}x{}) as size of({},{},{})\".format(len(x), len(x[0]), batch_size, sequence_length, feature_size))\n",
    "            x = x.view(batch_size, sequence_length, feature_size)\n",
    "        except:\n",
    "            print(x.shape)\n",
    "            break\n",
    "        hidden = model.init_hidden()\n",
    "        y_pred = model.forward(x, hidden)\n",
    "        print(\"\\nx={}\".format(x))\n",
    "        print(\"\\ny_pred={}\".format(y_pred))\n",
    "        loss = loss_fn(y_pred[-1], y[-1]) # pick y_pred very last value, y is also : out final result is y_pred[-1]\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"train, loss : {}\".format(loss))\n",
    "        tx_buf = []\n",
    "        rx_buf = []\n",
    "    else:\n",
    "        # save on buf\n",
    "        # TODO : make buf as like queue\n",
    "        # TODO : as is (chunk style), to be (sliding window style)\n",
    "        tx_buf.append(tx)\n",
    "        rx_buf.append(rx)\n",
    "        print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
