{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c01ad71-ae41-4707-9da6-dcc728b7cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021.12.04 / KwnakiAhn / How to setup pyspark on jupyuter\n",
    "\n",
    "# install java (https://m.blog.naver.com/opusk/220985259485)\n",
    "# !sudo apt-get install default-jdk\n",
    "\n",
    "# Install apache spark\n",
    "# !wget https://spark.apache.org/downloads.html \n",
    "# (wget https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz)\n",
    "# (wget https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz) / EMR release 6.0.0\n",
    "# !tar -xzf spark-1.2.0-bin-hadoop2.4.tgz\n",
    "# !sudo mv spark-3.2.0-bin-hadoop3.2 /opt/spark-3.2.0\n",
    "# !sudo ln -s /opt/spark-3.2.0 /opt/sparkÌ€\n",
    "# !export SPARK_HOME=/opt/spark\n",
    "# !export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "# install pyspark\n",
    "# !pip install pyspark\n",
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb7e26c-a28e-45e5-beac-37aa79672bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark-3.2.0/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jupyter/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jupyter/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a94181d7-aef0-43c4-a5fc-488715841510;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.2-spark3.2-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 232ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.2-spark3.2-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a94181d7-aef0-43c4-a5fc-488715841510\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/6ms)\n",
      "21/12/04 07:42:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/04 07:42:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "os.environ[\"SPARK_HOME\"] = ['/opt/spark-2.3.0', '/opt/spark-2.4.4', '/opt/spark-3.2.0'][2]\n",
    "sys.path.insert(0, os.environ[\"SPARK_HOME\"])\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# 2021.12.04 / KwankiAhn / How to use graphframes in pyspark\n",
    "# 1. config('spark.jars.packages') = pyspark --packages graphframes:graphframes:0.6.0-spark2.3-s_2.11\n",
    "# 2. graphframes <-> spark version sensitive!\n",
    "#  a. https://spark-packages.org/package/graphframes/graphframes\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"GraphFramePractice1\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .config('spark.jars.packages', 'graphframes:graphframes:0.8.2-spark3.2-s_2.12')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c36f9f-982f-4d0c-87c9-b3b35ab1f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutorial guide : https://graphframes.github.io/graphframes/docs/_site/quick-start.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "937a98af-42a7-4329-8d45-fc8401c27e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|inDegree|\n",
      "+---+--------+\n",
      "|  b|       2|\n",
      "|  c|       1|\n",
      "+---+--------+\n",
      "\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          pagerank|\n",
      "+---+------------------+\n",
      "|  c|1.8994109890559092|\n",
      "|  b|1.0905890109440908|\n",
      "|  a|              0.01|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Vertex DataFrame with unique ID column \"id\"\n",
    "sqlContext = spark\n",
    "v = sqlContext.createDataFrame([\n",
    "  (\"a\", \"Alice\", 34),\n",
    "  (\"b\", \"Bob\", 36),\n",
    "  (\"c\", \"Charlie\", 30),\n",
    "], [\"id\", \"name\", \"age\"])\n",
    "# Create an Edge DataFrame with \"src\" and \"dst\" columns\n",
    "e = sqlContext.createDataFrame([\n",
    "  (\"a\", \"b\", \"friend\"),\n",
    "  (\"b\", \"c\", \"follow\"),\n",
    "  (\"c\", \"b\", \"follow\"),\n",
    "], [\"src\", \"dst\", \"relationship\"])\n",
    "# Create a GraphFrame\n",
    "from graphframes import *\n",
    "g = GraphFrame(v, e)\n",
    "\n",
    "# Query: Get in-degree of each vertex.\n",
    "g.inDegrees.show()\n",
    "\n",
    "# Query: Count the number of \"follow\" connections in the graph.\n",
    "print(g.edges.filter(\"relationship = 'follow'\").count())\n",
    "\n",
    "# Run PageRank algorithm, and show results.\n",
    "results = g.pageRank(resetProbability=0.01, maxIter=20)\n",
    "results.vertices.select(\"id\", \"pagerank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1c837-99bd-4a33-8448-4a931df7c680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
